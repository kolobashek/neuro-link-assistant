import json
import logging
import os
import time
from typing import Any, Dict, Optional

import requests
from huggingface_hub import HfApi

from config import Config
from services.huggingface_service import HuggingFaceService

from .model_inference_service import ModelInferenceService

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
_model_inference_service = None


def get_model_inference_service():
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"""
    global _model_inference_service
    if _model_inference_service is None:
        _model_inference_service = ModelInferenceService()
    return _model_inference_service


logger = logging.getLogger("neuro_assistant")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å Hugging Face
hf_service = HuggingFaceService()

# –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö
MODELS_INFO_FILE = os.path.join(Config.DATA_DIR, "ai_models.json")


def get_ai_models():
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö AI-–º–æ–¥–µ–ª–µ–π

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö
        if not os.path.exists(MODELS_INFO_FILE):
            # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –µ–≥–æ —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
            create_default_models_file()

        # –ß–∏—Ç–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö –∏–∑ —Ñ–∞–π–ª–∞
        with open(MODELS_INFO_FILE, "r", encoding="utf-8") as f:
            models_data = json.load(f)

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
        current_model = get_current_ai_model()

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å "—Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å"
        for model in models_data.get("models", []):
            model["is_current"] = current_model and model["id"] == current_model["id"]

        return models_data
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ AI-–º–æ–¥–µ–ª–µ–π: {str(e)}")
        return {"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ AI-–º–æ–¥–µ–ª–µ–π: {str(e)}", "models": []}


def get_current_ai_model():
    """
    –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π –≤—ã–±—Ä–∞–Ω–Ω–æ–π AI-–º–æ–¥–µ–ª–∏

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ None
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö
        if not os.path.exists(MODELS_INFO_FILE):
            return None

        # –ß–∏—Ç–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö –∏–∑ —Ñ–∞–π–ª–∞
        with open(MODELS_INFO_FILE, "r", encoding="utf-8") as f:
            models_data = json.load(f)

        # –ò—â–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
        for model in models_data.get("models", []):
            if model.get("is_current", False):
                return model

        return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–µ–∫—É—â–µ–π AI-–º–æ–¥–µ–ª–∏: {str(e)}")
        return None


def check_ai_model_availability(model_id=None):
    """

    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π

    Args:

        model_id: ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:

        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        models_data = get_ai_models()
        models = models_data.get("models", [])

        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π ID –º–æ–¥–µ–ª–∏
        if model_id:
            # –ù–∞—Ö–æ–¥–∏–º –º–æ–¥–µ–ª—å –ø–æ ID
            model = next((m for m in models if m["id"] == model_id), None)

            if not model:
                return {"success": False, "message": f"–ú–æ–¥–µ–ª—å —Å ID {model_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"}

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –º–æ–¥–µ–ª—å—é OpenAI
            if (
                model.get("api_type") == "openai"
                or "openai" in model.get("huggingface_id", "").lower()
            ):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞ OpenAI
                if not os.environ.get("OPENAI_API_KEY"):
                    model["status"] = "unavailable"
                    model["error"] = (
                        "API –∫–ª—é—á OpenAI –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY."
                    )

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å
                    save_ai_models(models_data)

                    return {
                        "success": False,
                        "model_name": model["name"],
                        "message": model["error"],
                    }

                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ API OpenAI
                # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø—Ä–æ—Å—Ç–æ –æ—Ç–º–µ—Ç–∏–º –º–æ–¥–µ–ª—å –∫–∞–∫ –¥–æ—Å—Ç—É–ø–Ω—É—é
                model["status"] = "ready"
                model["error"] = None

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å
                save_ai_models(models_data)

                return {
                    "success": True,
                    "model_name": model["name"],
                    "message": f"–ú–æ–¥–µ–ª—å {model['name']} –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ API OpenAI",
                }

            # –î–ª—è –º–æ–¥–µ–ª–µ–π Hugging Face Hub
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
                from huggingface_hub import model_info

                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
                info = model_info(model["huggingface_id"])

                if info:
                    model["status"] = "ready"
                    model["error"] = None
                else:
                    model["status"] = "unavailable"
                    model["error"] = "–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ Hugging Face Hub"

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å
                save_ai_models(models_data)

                return {
                    "success": True,
                    "model_name": model["name"],
                    "message": (
                        "–ú–æ–¥–µ–ª—å"
                        f" {model['name']} {'–¥–æ—Å—Ç—É–ø–Ω–∞' if model['status'] == 'ready' else '–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}"
                    ),
                }
            except Exception as e:
                model["status"] = "unavailable"
                model["error"] = str(e)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å
                save_ai_models(models_data)

                return {
                    "success": False,
                    "model_name": model["name"],
                    "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–æ–¥–µ–ª–∏ {model['name']}: {str(e)}",
                }

        # –ï—Å–ª–∏ ID –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
        results = {
            "success": True,
            "message": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω–µ–Ω–∞",
            "models_checked": 0,
            "models_available": 0,
            "models_unavailable": 0,
        }

        for model in models:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª–∏ OpenAI
                if (
                    model.get("api_type") == "openai"
                    or "openai" in model.get("huggingface_id", "").lower()
                ):
                    if not os.environ.get("OPENAI_API_KEY"):
                        model["status"] = "unavailable"
                        model["error"] = "API –∫–ª—é—á OpenAI –Ω–µ –Ω–∞–π–¥–µ–Ω"
                        results["models_unavailable"] += 1
                    else:
                        model["status"] = "ready"
                        model["error"] = None
                        results["models_available"] += 1

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª–∏ Hugging Face Hub
                else:
                    try:
                        from huggingface_hub import model_info

                        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
                        info = model_info(model["huggingface_id"])

                        if info:
                            model["status"] = "ready"
                            model["error"] = None
                            results["models_available"] += 1
                        else:
                            model["status"] = "unavailable"
                            model["error"] = "–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ Hugging Face Hub"
                            results["models_unavailable"] += 1
                    except Exception as e:
                        model["status"] = "unavailable"
                        model["error"] = str(e)
                        results["models_unavailable"] += 1

                results["models_checked"] += 1
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–æ–¥–µ–ª–∏ {model.get('name', 'unknown')}: {str(e)}")
                model["status"] = "unavailable"
                model["error"] = str(e)
                results["models_unavailable"] += 1
                results["models_checked"] += 1

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        save_ai_models(models_data)

        return results
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π: {str(e)}")
        return {"success": False, "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π: {str(e)}"}


def select_ai_model(model_id):
    """
    –í—ã–±–∏—Ä–∞–µ—Ç AI-–º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

    Args:
        model_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –æ–ø–µ—Ä–∞—Ü–∏–∏
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        models_data = get_ai_models()

        if "error" in models_data:
            return {"success": False, "message": models_data["error"]}

        models = models_data.get("models", [])

        # –ò—â–µ–º –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º ID
        model = next((m for m in models if m["id"] == model_id), None)

        if not model:
            return {"success": False, "message": f"–ú–æ–¥–µ–ª—å —Å ID {model_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"}

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        model_inference = get_model_inference_service()
        availability = model_inference.check_model_availability(model_id)

        if not availability["available"]:
            return {
                "success": False,
                "message": (
                    f"–ú–æ–¥–µ–ª—å {model['name']} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞:"
                    f" {availability.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
                ),
            }

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        for m in models:
            m["is_current"] = m["id"] == model_id

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        models_data["models"] = models
        with open(MODELS_INFO_FILE, "w", encoding="utf-8") as f:
            json.dump(models_data, f, ensure_ascii=False, indent=2)

        # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        try:
            model_inference = get_model_inference_service()
            model_inference.load_model(model["huggingface_id"])
            model_inference.load_tokenizer(model["huggingface_id"])
        except Exception as e:
            logger.warning(f"–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model['name']} –Ω–µ —É–¥–∞–ª–∞—Å—å: {str(e)}")

        return {
            "success": True,
            "model_id": model_id,
            "model_name": model["name"],
            "message": f"–ú–æ–¥–µ–ª—å {model['name']} –≤—ã–±—Ä–∞–Ω–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è",
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ AI-–º–æ–¥–µ–ª–∏: {str(e)}")
        return {"success": False, "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ AI-–º–æ–¥–µ–ª–∏: {str(e)}"}


def update_model_status(model_id, status, error=None):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

    Args:
        model_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏
        status: –ù–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å (ready, busy, error, unavailable)
        error: –°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        models_data = get_ai_models()

        if "error" in models_data:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—Ç–∞—Ç—É—Å–∞ –º–æ–¥–µ–ª–∏: {models_data['error']}")
            return

        models = models_data.get("models", [])

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
        for model in models:
            if model["id"] == model_id:
                model["status"] = status
                if error:
                    model["error"] = error
                elif "error" in model:
                    del model["error"]
                break

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        models_data["models"] = models
        with open(MODELS_INFO_FILE, "w", encoding="utf-8") as f:
            json.dump(models_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—Ç–∞—Ç—É—Å–∞ –º–æ–¥–µ–ª–∏ {model_id}: {str(e)}")


def create_default_models_file():
    """
    –°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    """
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(os.path.dirname(MODELS_INFO_FILE), exist_ok=True)

        # –ë–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        default_models = {
            "models": [
                {
                    "id": "gpt-3.5-turbo",
                    "name": "GPT-3.5 Turbo",
                    "description": "–ú–æ–¥–µ–ª—å OpenAI GPT-3.5 Turbo (—Ç—Ä–µ–±—É–µ—Ç API –∫–ª—é—á OpenAI)",
                    "huggingface_id": "openai/gpt-3.5-turbo",
                    "type": "chat",
                    "status": "unavailable",
                    "is_current": True,
                    "api_type": "openai",  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø API
                },
                {
                    "id": "llama-2-7b",
                    "name": "Llama 2 (7B)",
                    "description": "–ú–æ–¥–µ–ª—å Meta Llama 2 (7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)",
                    "huggingface_id": "meta-llama/Llama-2-7b-hf",
                    "type": "completion",
                    "status": "unavailable",
                    "is_current": False,
                },
                {
                    "id": "mistral-7b",
                    "name": "Mistral 7B",
                    "description": "–ú–æ–¥–µ–ª—å Mistral AI (7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)",
                    "huggingface_id": "mistralai/Mistral-7B-v0.1",
                    "type": "completion",
                    "status": "unavailable",
                    "is_current": False,
                },
                {
                    "id": "gemma-7b",
                    "name": "Gemma 7B",
                    "description": "–ú–æ–¥–µ–ª—å Google Gemma (7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)",
                    "huggingface_id": "google/gemma-7b",
                    "type": "completion",
                    "status": "unavailable",
                    "is_current": False,
                },
            ]
        }

        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å Hugging Face Hub
        try:
            huggingface_models = get_available_huggingface_models(limit=5)

            # –î–æ–±–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª–∏ –≤ —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –¥—É–±–ª–∏—Ä—É—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            existing_ids = {model["huggingface_id"] for model in default_models["models"]}

            for model in huggingface_models:
                if model["huggingface_id"] not in existing_ids:
                    default_models["models"].append(model)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —Å Hugging Face Hub: {str(e)}")

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª
        with open(MODELS_INFO_FILE, "w", encoding="utf-8") as f:
            json.dump(default_models, f, ensure_ascii=False, indent=4)

        logger.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö: {MODELS_INFO_FILE}")
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞ —Å –º–æ–¥–µ–ª—è–º–∏: {str(e)}")
        return False


def generate_text(prompt, max_length=100, model_id=None):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

    Args:
        prompt: –ó–∞–ø—Ä–æ—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        model_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—É—â–∞—è)

    Returns:
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        if model_id:
            # –ò—â–µ–º –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º ID
            models_data = get_ai_models()
            models = models_data.get("models", [])
            model = next((m for m in models if m["id"] == model_id), None)

            if not model:
                raise ValueError(f"–ú–æ–¥–µ–ª—å —Å ID {model_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

            huggingface_id = model["huggingface_id"]
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
            current_model = get_current_ai_model()

            if not current_model:
                raise ValueError("–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å –Ω–µ –≤—ã–±—Ä–∞–Ω–∞")

            huggingface_id = current_model["huggingface_id"]
            model_id = current_model["id"]

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
        update_model_status(model_id, "busy")

        try:
            # ‚úÖ –ò–°–ü–û–õ–¨–ó–£–ï–ú ModelInferenceService –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            model_inference = get_model_inference_service()

            generated_text = model_inference.generate_text(
                model_id=huggingface_id,
                prompt=prompt,
                max_length=max_length,
                temperature=0.7,
                top_p=0.9,
            )

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
            update_model_status(model_id, "ready")

            return generated_text if generated_text else "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."

        except Exception as e:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
            update_model_status(model_id, "error", str(e))
            raise

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {str(e)}"


def generate_chat_response(messages, max_length=1000, model_id=None):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

    Args:
        messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "user", "content": "..."}, ...]
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        model_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—É—â–∞—è)

    Returns:
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        if model_id:
            # –ò—â–µ–º –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º ID
            models_data = get_ai_models()
            models = models_data.get("models", [])
            model = next((m for m in models if m["id"] == model_id), None)

            if not model:
                raise ValueError(f"–ú–æ–¥–µ–ª—å —Å ID {model_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

            huggingface_id = model["huggingface_id"]
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
            current_model = get_current_ai_model()

            if not current_model:
                raise ValueError("–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å –Ω–µ –≤—ã–±—Ä–∞–Ω–∞")

            huggingface_id = current_model["huggingface_id"]
            model_id = current_model["id"]

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
        update_model_status(model_id, "busy")

        try:
            # ‚úÖ –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π
            prompt = ""
            for message in messages:
                role = message.get("role", "user")
                content = message.get("content", "")

                if role == "system":
                    prompt += f"System: {content}\n\n"
                elif role == "user":
                    prompt += f"User: {content}\n\n"
                elif role == "assistant":
                    prompt += f"Assistant: {content}\n\n"

            prompt += "Assistant: "

            # ‚úÖ –ò–°–ü–û–õ–¨–ó–£–ï–ú ModelInferenceService –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            model_inference = get_model_inference_service()
            generated_text = model_inference.generate_text(
                model_id=huggingface_id,
                prompt=prompt,
                max_length=max_length,
                temperature=0.7,
                top_p=0.9,
            )

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
            update_model_status(model_id, "ready")

            return generated_text

        except Exception as e:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
            update_model_status(model_id, "error", str(e))
            raise

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –≤ —á–∞—Ç–µ: {str(e)}")
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"


def search_models(query, limit=20):
    """
    –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π –Ω–∞ Hugging Face Hub

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        list: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    """
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º API
        from huggingface_hub import HfApi

        api = HfApi()

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π
        models = api.list_models(
            search=query,  # –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            filter="text-generation",  # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –º–æ–¥–µ–ª–∏
            sort="downloads",  # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–≥—Ä—É–∑–æ–∫
            direction=-1,  # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é
            limit=limit,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        result = []
        for model in models:
            result.append(
                {
                    "id": model.id,
                    "name": model.id.split("/")[-1],
                    "author": model.id.split("/")[0] if "/" in model.id else "Unknown",
                    "description": (
                        model.card_data.get("description", "") if model.card_data else ""
                    ),
                    "tags": model.tags,
                    "downloads": model.downloads,
                    "likes": model.likes,
                }
            )

        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –º–æ–¥–µ–ª–µ–π: {str(e)}")
        return []


def add_model(model_data):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö

    Args:
        model_data: –î–∞–Ω–Ω—ã–µ –æ –º–æ–¥–µ–ª–∏ (id, name, description, huggingface_id, type)

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –æ–ø–µ—Ä–∞—Ü–∏–∏
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        required_fields = ["id", "name", "huggingface_id", "type"]
        for field in required_fields:
            if field not in model_data:
                return {"success": False, "message": f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ: {field}"}

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        models_data = get_ai_models()

        if "error" in models_data:
            return {"success": False, "message": models_data["error"]}

        models = models_data.get("models", [])

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —Å —Ç–∞–∫–∏–º ID –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if any(m["id"] == model_data["id"] for m in models):
            return {"success": False, "message": f"–ú–æ–¥–µ–ª—å —Å ID {model_data['id']} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"}

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face
        model_inference = get_model_inference_service()
        availability = model_inference.check_model_availability(model_data.model_id)

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
        new_model = {
            "id": model_data["id"],
            "name": model_data["name"],
            "description": model_data.get("description", ""),
            "huggingface_id": model_data["huggingface_id"],
            "type": model_data["type"],
            "status": "ready" if availability["available"] else "unavailable",
            "is_current": False,
        }

        if not availability["available"] and "error" in availability:
            new_model["error"] = availability["error"]

        # –î–æ–±–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –≤ —Å–ø–∏—Å–æ–∫
        models.append(new_model)
        models_data["models"] = models

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        with open(MODELS_INFO_FILE, "w", encoding="utf-8") as f:
            json.dump(models_data, f, ensure_ascii=False, indent=2)

        return {
            "success": True,
            "model": new_model,
            "message": f"–ú–æ–¥–µ–ª—å {new_model['name']} —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞",
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
        return {"success": False, "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {str(e)}"}


def remove_model(model_id):
    """
    –£–¥–∞–ª—è–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö

    Args:
        model_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –æ–ø–µ—Ä–∞—Ü–∏–∏
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        models_data = get_ai_models()

        if "error" in models_data:
            return {"success": False, "message": models_data["error"]}

        models = models_data.get("models", [])

        # –ò—â–µ–º –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º ID
        model = next((m for m in models if m["id"] == model_id), None)

        if not model:
            return {"success": False, "message": f"–ú–æ–¥–µ–ª—å —Å ID {model_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"}

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å —Ç–µ–∫—É—â–µ–π
        if model.get("is_current", False):
            return {
                "success": False,
                "message": "–ù–µ–ª—å–∑—è —É–¥–∞–ª–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å. –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å.",
            }

        # –£–¥–∞–ª—è–µ–º –º–æ–¥–µ–ª—å –∏–∑ —Å–ø–∏—Å–∫–∞
        models = [m for m in models if m["id"] != model_id]
        models_data["models"] = models

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        with open(MODELS_INFO_FILE, "w", encoding="utf-8") as f:
            json.dump(models_data, f, ensure_ascii=False, indent=2)

        return {
            "success": True,
            "model_id": model_id,
            "message": f"–ú–æ–¥–µ–ª—å {model['name']} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞",
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
        return {"success": False, "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {str(e)}"}


def get_ai_response(prompt, system_message=None):
    """
    –ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç AI-–º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ HuggingFace Inference API

    Args:
        prompt: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        system_message: –°–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        –û—Ç–≤–µ—Ç –æ—Ç AI-–º–æ–¥–µ–ª–∏
    """
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º HuggingFace Inference API
        hf_response = get_huggingface_response(prompt, system_message)
        if hf_response:
            return hf_response

        # –ï—Å–ª–∏ HF –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
        current_model = get_current_ai_model()
        if not current_model:
            return "–û—à–∏–±–∫–∞: –ù–µ –≤—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å AI. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö."

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —á–∞—Ç–∞
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": prompt})

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –∏ –≤—ã–∑—ã–≤–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é
        if current_model.get("type") == "chat":
            return generate_chat_response(messages)
        else:
            # –î–ª—è –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ completion —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –ø–æ-–¥—Ä—É–≥–æ–º—É
            full_prompt = ""
            if system_message:
                full_prompt += f"{system_message}\n\n"
            full_prompt += f"User: {prompt}\nAssistant: "
            return generate_text(full_prompt)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI: {str(e)}")
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI: {str(e)}"


def get_huggingface_response(messages: list, debug_info: dict | None = None) -> Optional[str]:
    """Inference API —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º endpoint –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    if debug_info is None:
        debug_info = {}

    try:
        api_key = Config.HUGGINGFACE_TOKEN
        debug_info["api_key_check"] = "PRESENT" if api_key else "MISSING"

        if not api_key:
            debug_info["errors"] = debug_info.get("errors", [])
            debug_info["errors"].append("HUGGINGFACE_TOKEN –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            return None

        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π endpoint –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        api_url = "https://router.huggingface.co/novita/v3/openai/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

        payload = {"messages": messages, "model": "deepseek/deepseek-v3-0324", "stream": False}

        debug_info["request"] = {
            "url": api_url,
            "model": payload["model"],
            "messages_count": len(messages),
        }

        response = requests.post(api_url, headers=headers, json=payload, timeout=30)

        debug_info["response"] = {
            "status": response.status_code,
            "text_preview": response.text[:200],
        }

        if response.status_code == 200:
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                debug_info["success"] = True
                return result["choices"][0]["message"]["content"]

        debug_info["errors"] = debug_info.get("errors", [])
        debug_info["errors"].append(f"HTTP {response.status_code}: {response.text}")

    except Exception as e:
        debug_info["exception"] = str(e)

    return None


def _call_huggingface_api(model_name: str, prompt: str, max_retries: int = 3) -> Optional[str]:
    """
    –í—ã–∑–æ–≤ HuggingFace Inference API –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏

    Args:
        model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace
        prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
        max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫

    Returns:
        –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –∏–ª–∏ None
    """
    api_url = f"https://api-inference.huggingface.co/models/{model_name}"
    logger.info(f"üåê URL –∑–∞–ø—Ä–æ—Å–∞: {api_url}")

    headers = {}
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if Config.HUGGINGFACE_TOKEN:
        headers["Authorization"] = f"Bearer {Config.HUGGINGFACE_TOKEN}"
        logger.info("üîë –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–æ–±–∞–≤–ª–µ–Ω")
    else:
        logger.warning("‚ö†Ô∏è –¢–æ–∫–µ–Ω –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_length": 150,
            "temperature": 0.7,
            "do_sample": True,
            "top_p": 0.9,
            "repetition_penalty": 1.1,
        },
        "options": {"wait_for_model": True, "use_cache": False},
    }

    logger.info(f"üì¶ Payload: {payload}")

    for attempt in range(max_retries):
        try:
            logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")

            response = requests.post(api_url, headers=headers, json=payload, timeout=30)

            logger.info(f"üìä –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {response.status_code}")
            logger.info(f"üìã –ó–∞–≥–æ–ª–æ–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞: {dict(response.headers)}")

            if response.status_code == 200:
                result = response.json()
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π JSON –æ—Ç–≤–µ—Ç: {result}")

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤
                if isinstance(result, list) and len(result) > 0:
                    if "generated_text" in result[0]:
                        generated = result[0]["generated_text"]
                        # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
                        if generated.startswith(prompt):
                            generated = generated[len(prompt) :].strip()
                        logger.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {generated}")
                        return generated[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                    elif "text" in result[0]:
                        return result[0]["text"][:500]
                elif isinstance(result, dict):
                    if "generated_text" in result:
                        return result["generated_text"][:500]
                    elif "text" in result:
                        return result["text"][:500]

                logger.warning("‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞")
                return "–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞"

            elif response.status_code == 503:
                logger.info(f"‚è≥ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è, –∂–¥–µ–º...")
                time.sleep(5)
                continue

            elif response.status_code == 429:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}")
                time.sleep(2)
                continue

            else:
                error_text = response.text
                logger.error(f"‚ùå HuggingFace API –æ—à–∏–±–∫–∞ {response.status_code}: {error_text}")
                return None

        except requests.exceptions.Timeout:
            logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
        except requests.exceptions.RequestException as e:
            logger.error(f"üåê –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –¥–ª—è {model_name}: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è {model_name}: {e}")
            return None

    logger.error(f"‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}")
    return None


def get_simple_ai_response(
    messages: list, model_name: str | None = None, debug_info: dict | None = None
) -> str:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –±–µ–∑ fallback –æ—Ç–≤–µ—Ç–æ–≤"""
    if debug_info is None:
        debug_info = {"errors": []}

    try:
        api_key = Config.HUGGINGFACE_TOKEN
        if not api_key:
            debug_info["errors"].append("HUGGINGFACE_TOKEN –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return "‚ùå AI —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
        current_model = get_current_ai_model()
        if not current_model:
            debug_info["errors"].append("–ê–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –≤—ã–±—Ä–∞–Ω–∞")
            return "‚ùå AI —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: –º–æ–¥–µ–ª—å –Ω–µ –≤—ã–±—Ä–∞–Ω–∞"

        if current_model.get("status") != "ready":
            debug_info["errors"].append(f"–ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {current_model.get('status')}")
            return f"‚ùå AI —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: –º–æ–¥–µ–ª—å '{current_model.get('name')}' –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"

        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å –∫ HuggingFace
        hf_response = get_huggingface_response(messages, debug_info=debug_info)
        if hf_response and hf_response.strip():
            debug_info["success"] = True
            return hf_response

        debug_info["errors"].append("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç AI")
        return "‚ùå AI —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"

    except Exception as e:
        debug_info["errors"].append(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {str(e)}")
        return f"‚ùå –û—à–∏–±–∫–∞ AI —Å–µ—Ä–≤–∏—Å–∞: {str(e)}"


def get_fallback_response(messages: list) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –∫–æ–≥–¥–∞ AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_message = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                user_message = msg.get("content", "").lower()
                break

        # –ü—Ä–æ—Å—Ç—ã–µ —à–∞–±–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
        responses = {
            "–ø—Ä–∏–≤–µ—Ç": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
            "–∫–∞–∫ –¥–µ–ª–∞": "–í—Å—ë —Ö–æ—Ä–æ—à–æ, —Å–ø–∞—Å–∏–±–æ! –ê —É —Ç–µ–±—è –∫–∞–∫?",
            "—á—Ç–æ —É–º–µ–µ—à—å": (
                "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏. AI —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ —è —Å—Ç–∞—Ä–∞—é—Å—å"
                " –æ—Ç–≤–µ—á–∞—Ç—å!"
            ),
            "—Å–ø–∞—Å–∏–±–æ": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞! –í—Å–µ–≥–¥–∞ —Ä–∞–¥ –ø–æ–º–æ—á—å.",
            "–ø–æ–∫–∞": "–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! –£–¥–∞—á–Ω–æ–≥–æ –¥–Ω—è!",
        }

        # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç
        for key, response in responses.items():
            if key in user_message:
                return (
                    f"ü§ñ {response}\n\n*–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: AI —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è"
                    " –±–∞–∑–æ–≤—ã–π —Ä–µ–∂–∏–º.*"
                )

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç
        return (
            f"ü§ñ –ü–æ–Ω—è–ª –≤–∞—à –≤–æ–ø—Ä–æ—Å –ø—Ä–æ '{user_message[:50]}...'. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, AI —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ"
            " –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ —è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–≤–µ—á—É, –∫–æ–≥–¥–∞ –æ–Ω –∑–∞—Ä–∞–±–æ—Ç–∞–µ—Ç!\n\n*–ü–æ–ø—Ä–æ–±—É–π—Ç–µ"
            " –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –ø–æ–∑–∂–µ.*"
        )

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ get_fallback_response: {str(e)}")
        return "ü§ñ –ò–∑–≤–∏–Ω–∏—Ç–µ, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–µ–ø–æ–ª–∞–¥–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ!"


def get_available_huggingface_models(filter_criteria=None, limit=20):
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å Hugging Face Hub

    Args:
        filter_criteria: –ö—Ä–∏—Ç–µ—Ä–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è

    Returns:
        –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
    """
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º API
        api = HfApi()

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        models = api.list_models(
            filter=filter_criteria or "text-generation",  # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –º–æ–¥–µ–ª–∏
            sort="downloads",  # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–≥—Ä—É–∑–æ–∫
            direction=-1,  # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é
            limit=limit,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        result = []
        for model in models:
            model_id = model.id.replace("/", "-").lower()  # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π ID
            result.append(
                {
                    "id": model_id,
                    "name": model.id.split("/")[-1],
                    "description": (
                        model.card_data.get("description", "")
                        if model.card_data
                        else f"–ú–æ–¥–µ–ª—å {model.id}"
                    ),
                    "huggingface_id": model.id,
                    "type": (
                        "completion"
                    ),  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
                    "status": "unavailable",
                    "is_current": False,
                }
            )

        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π —Å Hugging Face Hub: {str(e)}")
        return []


def update_models_from_huggingface():
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π, –¥–æ–±–∞–≤–ª—è—è –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å Hugging Face Hub

    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –º–æ–¥–µ–ª—è–º–∏
        if not os.path.exists(MODELS_INFO_FILE):
            create_default_models_file()

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        with open(MODELS_INFO_FILE, "r", encoding="utf-8") as f:
            models_data = json.load(f)

        current_models = models_data.get("models", [])

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å Hugging Face Hub
        huggingface_models = get_available_huggingface_models(limit=10)

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        existing_models = {model["huggingface_id"]: True for model in current_models}

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –∏—Ö –µ—â–µ –Ω–µ—Ç –≤ —Å–ø–∏—Å–∫–µ
        added_count = 0
        for hf_model in huggingface_models:
            if hf_model["huggingface_id"] not in existing_models:
                current_models.append(hf_model)
                added_count += 1

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        with open(MODELS_INFO_FILE, "w", encoding="utf-8") as f:
            json.dump({"models": current_models}, f, ensure_ascii=False, indent=4)

        return {
            "success": True,
            "message": f"–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –æ–±–Ω–æ–≤–ª–µ–Ω. –î–æ–±–∞–≤–ª–µ–Ω–æ {added_count} –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
            "added_count": added_count,
            "total_models": len(current_models),
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {str(e)}")
        return {"success": False, "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {str(e)}"}


def save_ai_models(models_data):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö –≤ —Ñ–∞–π–ª

    Args:
        models_data: –î–∞–Ω–Ω—ã–µ –æ –º–æ–¥–µ–ª—è—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

    Returns:
        bool: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏
    """
    try:
        with open(MODELS_INFO_FILE, "w", encoding="utf-8") as f:
            json.dump(models_data, f, ensure_ascii=False, indent=4)
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö: {str(e)}")
        return False
